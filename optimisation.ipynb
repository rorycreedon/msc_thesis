{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SoftSort(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class that implements differentiable soft sorting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tau: float = 1.0, hard: bool = False, power: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        :param tau: temperature parameter\n",
    "        :param hard: whether to use soft or hard sorting\n",
    "        :param power: power to use in the semi-metric d\n",
    "        \"\"\"\n",
    "        super(SoftSort, self).__init__()\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, scores: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the class\n",
    "        :param scores: The scores to be sorted\n",
    "        :return: The softmax of sorted scores (descending sort)\n",
    "        \"\"\"\n",
    "        scores = scores.unsqueeze(-1)\n",
    "        sorted = scores.sort(descending=False, dim=1)[0]\n",
    "        pairwise_diff = (scores.transpose(1, 2) - sorted).abs().pow(\n",
    "            self.power\n",
    "        ).neg() / self.tau\n",
    "        P_hat = pairwise_diff.softmax(-1)\n",
    "\n",
    "        if self.hard:\n",
    "            P = torch.zeros_like(P_hat, device=P_hat.device)\n",
    "            P.scatter_(-1, P_hat.topk(1, -1)[1], value=1)\n",
    "            P_hat = (P - P_hat).detach() + P_hat\n",
    "        return P_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_non_diff(\n",
    "    A: torch.Tensor,\n",
    "    X: torch.Tensor,\n",
    "    O: torch.Tensor,\n",
    "    W: torch.Tensor,\n",
    "    betas: torch.Tensor,\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Non differentiable loss function to measure cost (uses argsort)\n",
    "    :param A: Actions to take for each variable\n",
    "    :param X: The original values of each feature\n",
    "    :param O: The initial ordering of the features\n",
    "    :param W: The weighted adjacency matrix\n",
    "    :param betas: The relative mutability of each feature\n",
    "    :return: (X_bar, cost) where X_bar is the sorted values and cost total cost of applying A with ordering O.\n",
    "    \"\"\"\n",
    "    cost = 0.0\n",
    "    S = torch.argsort(O)\n",
    "    X_bar = X.clone()\n",
    "    # add 1 to the diagonal of W\n",
    "    W = W + torch.eye(W.size(0))\n",
    "    for i in S:\n",
    "        X_bar += A[i] * W[:, i]\n",
    "        cost += (A[i] ** 2) * torch.sigmoid(betas[i])\n",
    "    return X_bar, cost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def loss_differentiable(\n",
    "    A: torch.Tensor,\n",
    "    X: torch.Tensor,\n",
    "    O: torch.Tensor,\n",
    "    W: torch.Tensor,\n",
    "    betas: torch.Tensor,\n",
    "    sorter,\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Differentiable loss function to measure cost (uses softsort)\n",
    "    :param A: Actions to take for each variable\n",
    "    :param X: The original values of each feature\n",
    "    :param O: The initial ordering of the features\n",
    "    :param W: The weighted adjacency matrix\n",
    "    :param betas: The relative mutability of each feature\n",
    "    :param sorter: The softsort-ing function\n",
    "    :return: (X_bar, cost) where X_bar is the sorted values and cost total cost of applying A with ordering O.\n",
    "    \"\"\"\n",
    "    # Number of individuals\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Initialize result tensors\n",
    "    X_bars = torch.zeros(X.shape)\n",
    "    W_temp = W + torch.eye(W.shape[0])\n",
    "    if O.dim()==1:\n",
    "        S = sorter(O.unsqueeze(0))\n",
    "    else:\n",
    "        S = sorter(O)\n",
    "    cost = torch.zeros(X.shape[0])\n",
    "\n",
    "    # Iterate over each row of A, X, and O\n",
    "    for n in range(N):\n",
    "        X_bar = X[n].clone()\n",
    "\n",
    "        for i in range(W.shape[0]):\n",
    "            X_bar += (W_temp * S[n][i]) @ A[n]\n",
    "            cost[n] += torch.sum(A[n] ** 2 * S[n][i] * torch.sigmoid(betas))\n",
    "\n",
    "        # Store results for this row\n",
    "        X_bars[n] = X_bar\n",
    "\n",
    "    return X_bars, cost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T15:11:30.060137001Z",
     "start_time": "2023-08-11T15:11:30.018750525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SETTING UP PARAMETERS TO BE OPTIMIZED\n",
    "A = torch.tensor([[0, 0, 0, 0], [0, 0, 0, 0]], dtype=torch.float32, requires_grad=True)\n",
    "O = torch.rand(2, 4, dtype=torch.float32, requires_grad=True)\n",
    "C = torch.rand(1, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# FIXED PARAMETERS\n",
    "beta = torch.tensor([1, 1, 1, 1], dtype=torch.float32, requires_grad=True)\n",
    "X = torch.tensor([[-1, -2, -5, -1], [-3, -6, 3, -6]], dtype=torch.float32)\n",
    "W_adjacency = torch.tensor(\n",
    "    [[0, 0, 0, 0], [0.3, 0, 0, 0], [0.2, 0, 0, 0], [0, 0.2, 0.3, 0]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_classifier = torch.tensor([2, 3, 1, 4], dtype=torch.float32)\n",
    "sorter = SoftSort(tau=0.1, hard=True)\n",
    "\n",
    "# Work out initial X_bar and cost\n",
    "X_bar, cost = loss_differentiable(A, X, O, W_classifier, beta, sorter)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def optimise(\n",
    "    max_optimiser: torch.optim,\n",
    "    min_optimiser: torch.optim,\n",
    "    n_epochs: int,\n",
    "    A: torch.Tensor,\n",
    "    X: torch.Tensor,\n",
    "    O: torch.Tensor,\n",
    "    C: torch.Tensor,\n",
    "    W_adjacency: torch.Tensor,\n",
    "    beta: torch.Tensor,\n",
    "    sorter: SoftSort,\n",
    "    classifier_margin: float,\n",
    "):\n",
    "    assert (\n",
    "        classifier_margin >= 0\n",
    "    ), \"Classifier margin must be greater than or equal to 0\"\n",
    "\n",
    "    # Create lists\n",
    "    objective_list = []\n",
    "    constraint_list = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # Maximise wrt C\n",
    "        X_bar, cost = loss_differentiable(A, X, O, W_adjacency, beta, sorter)\n",
    "        constraint = (\n",
    "            loss_differentiable(A, X, O, W_adjacency, beta, sorter)[0] @ W_classifier\n",
    "            - classifier_margin\n",
    "        )\n",
    "        max_loss = (C * constraint) - cost\n",
    "\n",
    "        max_optimiser.zero_grad()\n",
    "        max_loss.backward()\n",
    "        max_optimiser.step()\n",
    "\n",
    "        # Minimise wrt A, O, beta\n",
    "        X_bar, cost = loss_differentiable(A, X, O, W_adjacency, beta, sorter)\n",
    "        constraint = (\n",
    "            loss_differentiable(A, X, O, W_adjacency, beta, sorter)[0] @ W_classifier\n",
    "            - classifier_margin\n",
    "        )\n",
    "        min_loss = cost - (C * constraint)\n",
    "\n",
    "        min_optimiser.zero_grad()\n",
    "        min_loss.backward()\n",
    "        min_optimiser.step()\n",
    "\n",
    "        # Track objective and constraints\n",
    "        objective_list.append(cost.item())\n",
    "        constraint_list.append(constraint.item())\n",
    "\n",
    "        # Early stopping\n",
    "        if (\n",
    "            i > 100\n",
    "            and np.std(objective_list[-10:]) < 1e-4\n",
    "            and np.std(constraint_list[-10:]) < 1e-4\n",
    "        ):\n",
    "            break\n",
    "            \n",
    "    # Print final ordering\n",
    "    ordering = torch.max(sorter(O), dim=1)[1]\n",
    "\n",
    "    # Return results\n",
    "    return X_bar, ordering, cost, constraint"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T15:18:00.947722137Z",
     "start_time": "2023-08-11T15:18:00.926459716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.1744, -0.2081, -3.9317,  1.0533], grad_fn=<AddBackward0>), tensor([[3, 2, 0, 1]]), tensor(4.7096, grad_fn=<AddBackward0>), tensor(0.0056, grad_fn=<AddBackward0>))\n",
      "(tensor([ 0.1085, -1.2570,  5.8278, -0.5651], grad_fn=<AddBackward0>), tensor([[0, 1, 3, 2]]), tensor(32.9984, grad_fn=<AddBackward0>), tensor(0.0138, grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "def optimise(\n",
    "        max_optimiser: torch.optim,\n",
    "        min_optimiser: torch.optim,\n",
    "        n_epochs: int,\n",
    "        A: torch.Tensor,\n",
    "        X: torch.Tensor,\n",
    "        O: torch.Tensor,\n",
    "        C: torch.Tensor,\n",
    "        W_adjacency: torch.Tensor,\n",
    "        W_classifier: torch.Tensor,\n",
    "        beta: torch.Tensor,\n",
    "        sorter: SoftSort,\n",
    "        classifier_margin: float,\n",
    "):\n",
    "    assert (\n",
    "            classifier_margin >= 0\n",
    "    ), \"Classifier margin must be greater than or equal to 0\"\n",
    "\n",
    "    # Create lists\n",
    "    objective_list = []\n",
    "    constraint_list = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # Maximise wrt C\n",
    "        X_bar, cost = loss_differentiable(A, X, O, W_adjacency, beta, sorter)\n",
    "        constraint = (\n",
    "                loss_differentiable(A, X, O, W_adjacency, beta, sorter)[0] @ W_classifier\n",
    "                - classifier_margin\n",
    "        )\n",
    "        max_loss = (C * constraint) - cost\n",
    "\n",
    "        max_optimiser.zero_grad()\n",
    "        max_loss.backward()\n",
    "        max_optimiser.step()\n",
    "\n",
    "        # Minimise wrt A, O, beta\n",
    "        X_bar, cost = loss_differentiable(A, X, O, W_adjacency, beta, sorter)\n",
    "        constraint = (\n",
    "                loss_differentiable(A, X, O, W_adjacency, beta, sorter)[0] @ W_classifier\n",
    "                - classifier_margin\n",
    "        )\n",
    "        min_loss = cost - (C * constraint)\n",
    "\n",
    "        min_optimiser.zero_grad()\n",
    "        min_loss.backward()\n",
    "        min_optimiser.step()\n",
    "\n",
    "        # Track objective and constraints\n",
    "        objective_list.append(cost.item())\n",
    "        constraint_list.append(constraint.item())\n",
    "\n",
    "        # Early stopping\n",
    "        if (\n",
    "                i > 100\n",
    "                and np.std(objective_list[-10:]) < 1e-4\n",
    "                and np.std(constraint_list[-10:]) < 1e-4\n",
    "        ):\n",
    "            break\n",
    "\n",
    "    # Print final ordering\n",
    "    ordering = torch.max(sorter(O.unsqueeze(0)), dim=1)[1]\n",
    "\n",
    "    # Return results\n",
    "    return X_bar, ordering, cost, constraint + classifier_margin\n",
    "\n",
    "def loss_differentiable(\n",
    "        A: torch.Tensor,\n",
    "        X: torch.Tensor,\n",
    "        O: torch.Tensor,\n",
    "        W: torch.Tensor,\n",
    "        betas: torch.Tensor,\n",
    "        sorter,\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Differentiable loss function to measure cost (uses softsort)\n",
    "    :param A: Actions to take for each variable\n",
    "    :param X: The original values of each feature\n",
    "    :param O: The initial ordering of the features\n",
    "    :param W: The weighted adjacency matrix\n",
    "    :param betas: The relative mutability of each feature\n",
    "    :param sorter: The softsort-ing function\n",
    "    :return: (X_bar, cost) where X_bar is the sorted values and cost total cost of applying A with ordering O.\n",
    "    \"\"\"\n",
    "    # Number of individuals\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Initialize result tensors\n",
    "    X_bar = X.clone()\n",
    "    W_temp = W + torch.eye(W.shape[0])\n",
    "    if O.dim()==1:\n",
    "        S = sorter(O.unsqueeze(0))[0]\n",
    "    else:\n",
    "        S = sorter(O)[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        X_bar += (W_temp * S[i]) @ A\n",
    "        cost += torch.sum(A ** 2 * S[i] * torch.sigmoid(betas))\n",
    "\n",
    "    return X_bar, cost\n",
    "\n",
    "# SETTING UP PARAMETERS TO BE OPTIMIZED\n",
    "As = [torch.tensor([0, 0, 0, 0], dtype=torch.float32, requires_grad=True) for _ in range(2)]\n",
    "O = torch.rand(2, 4, dtype=torch.float32, requires_grad=True)\n",
    "Os = [torch.rand(4, dtype=torch.float32, requires_grad=True) for _ in range(2)]\n",
    "C = torch.rand(1, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# FIXED PARAMETERS\n",
    "beta = torch.tensor([1, 1, 1, 1], dtype=torch.float32, requires_grad=True)\n",
    "X = torch.tensor([[-1, -2, -5, -1], [-3, -6, 3, -6]], dtype=torch.float32)\n",
    "W_adjacency = torch.tensor(\n",
    "    [[0, 0, 0, 0], [0.3, 0, 0, 0], [0.2, 0, 0, 0], [0, 0.2, 0.3, 0]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_classifier = torch.tensor([2, 3, 1, 4], dtype=torch.float32)\n",
    "sorter = SoftSort(tau=0.1, hard=True)\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    max_optimiser = optim.SGD([C], lr=1e-2)\n",
    "    max_optimiser.zero_grad()\n",
    "    min_optimiser = optim.SGD(\n",
    "        [\n",
    "            {\"params\": [As[i]], \"lr\": 1e-2},\n",
    "            {\"params\": [Os[i]], \"lr\": 1e-2},\n",
    "        ]\n",
    "    )\n",
    "    min_optimiser.zero_grad()\n",
    "\n",
    "    print(optimise(\n",
    "        max_optimiser = max_optimiser,\n",
    "        min_optimiser = min_optimiser,\n",
    "        n_epochs = 2_000,\n",
    "        A = As[i],\n",
    "        X = X[i],\n",
    "        O = Os[i],\n",
    "        C = C,\n",
    "        W_adjacency = W_adjacency,\n",
    "        W_classifier = W_classifier,\n",
    "        beta = beta,\n",
    "        sorter = sorter,\n",
    "        classifier_margin = 0.01,\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T15:39:12.510456386Z",
     "start_time": "2023-08-11T15:39:07.154831170Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.8037, 0.0326, 0.0135, 0.4695],\n        [0.7466, 0.7928, 0.5113, 0.3977]], requires_grad=True)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T16:29:39.816197916Z",
     "start_time": "2023-08-11T16:29:39.768921136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(sorter(O), dim=1)[0].detach().squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T16:30:19.152439808Z",
     "start_time": "2023-08-11T16:30:19.054920914Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
