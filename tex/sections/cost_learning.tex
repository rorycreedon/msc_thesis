\chapter{Cost Learning}

In order to generate recourse selections, we need to solve the constrained optimisation problem mentioned in equation \ref{eq:recourse_setup}, where $\mathbf{x}$ are the individual's original features, $f$ is the utility of being positively or negatively classified, $c$ is the cost function and $B$ is the individual's `budget' for changing their features.

\begin{align}
	& \mathbf{x}^f = \argmax_{\mathbf{x}' \in \mathbf{\mathcal{X}}} f(\mathbf{x}) - c(\mathbf{x}, \mathbf{x}') \\
	& \text{s.t. } c(\mathbf{x}, \mathbf{x}') \leq B \nonumber
\end{align}

To solve for $\mathbf{x}^f$ effectively, this is typically handled as a convex optimisation problem. This requires the learned cost function $c$ to be suitable to be convex/suitable for convex optimisation. Two different functional forms for the cost function are outlined below.


\section{Mahalanobis distance}

\subsection{Overview of Mahalanobis distance}

The Mahalanobis distance between the vector $\mathbf{x}$ and the vector $\mathbf{y}$ is defined in equation \ref{eq:mahalanbobis_distance}, where $M$ is a positive semi-definite matrix.

\begin{equation} \label{eq:mahalanbobis_distance}
	||\mathbf{x-y}||_{\mathbf{M}} = \sqrt{(\mathbf{x-y})^T\mathbf{M}^{-1}(\mathbf{x-y})}
\end{equation}

The matrix $\mathbf{M}$ captures different distances relationships between the features within $\mathbf{x}$ and $\mathbf{y}$ in the off-diagonal elements of $\mathbf{M}$. If $\mathbf{M}$ is set to the identity matrix, then the Mahalanobis distance then becomes equal to the Euclidean distance between $\mathbf{x}$ and $\mathbf{y}$. \\

\subsection{Learning the Mahalanobis distance}

In order to use the Mahalanobis distance as a cost function, we must learn the matrix $\mathbf{M}$. In this set-up, each individual $k$ with original features $\mathbf{x}_k$ is presented with $N$ recourse options $(\mathbf{x}_{kn}^a, \mathbf{x}_{kn}^b)$ and responds with $y_{kn}=-1$ if offering $a$ is preferred (preferences are defined by the ground truth cost function) and $y_{kn}=1$ if offering $b$ is preferred. The optimisation problem presented in \textcite{canalOneAllSimultaneous2022} is simplified (to only conduct metric learning, as opposed to metric and preference learning) in equation \ref{eq:mahalanobis_optimisation}, where $\ell$ represents either the hinge or logistic loss function.

\begin{align} \label{eq:mahalanobis_optimisation}
	\min_{\mathbf{M}} & \frac{1}{KN} \sum_{k=1}^K \sum_{n=1}^N \ell \bigg( y_{kn} \big(|| \mathbf{x}_k - \mathbf{x}_{kn}^a ||^2_{\mathbf{M}} - || \mathbf{x}_k - \mathbf{x}_{kn}^b ||^2_{\mathbf{M}} \big) \bigg) \\	
	\text{s.t. } & \mathbf{M} \succeq 0, \nonumber \\
	& ||\mathbf{M}||_F \leq \lambda_F \nonumber
\end{align}

The term $\lambda_F$ is used to regularise the matrix $\mathbf{M}$. This is a convex problem that can be solved using an convex optimisation solver such as SCS \citep{odonoghueOperatorSplittingHomogeneous2021}.



\section{Convex layers}

To look into convex neural networks using \href{https://github.com/cvxgrp/cvxpylayers}{\texttt{cvxpylayers}}, which is based on \textcite{agrawalDifferentiableConvexOptimization2019}.
