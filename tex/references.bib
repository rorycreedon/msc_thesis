@inproceedings{agrawalDifferentiableConvexOptimization2019,
  title = {Differentiable {{Convex Optimization Layers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32: {{Annual Conference}} on {{Neural Information Processing Systems}} 2019, {{NeurIPS}} 2019, {{December}} 8-14, 2019, {{Vancouver}}, {{BC}}, {{Canada}}},
  author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane T. and Boyd, Stephen P. and Diamond, Steven and Kolter, J. Zico},
  date = {2019},
  pages = {9558--9570},
  url = {https://proceedings.neurips.cc/paper/2019/hash/9ce3c52fc54362e22053399d3181c638-Abstract.html},
  urldate = {2023-07-14}
}

@inproceedings{ahmadiClassificationStrategicAgents2022,
  title = {On {{Classification}} of {{Strategic Agents Who Can Both Game}} and {{Improve}}},
  booktitle = {3rd {{Symposium}} on {{Foundations}} of {{Responsible Computing}} ({{FORC}} 2022)},
  author = {Ahmadi, Saba and Beyhaghi, Hedyeh and Blum, Avrim and Naggita, Keziah},
  date = {2022},
  volume = {218},
  pages = {3:1--3:22},
  publisher = {{Schloss Dagstuhl – Leibniz-Zentrum für Informatik}},
  location = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.FORC.2022.3},
  urldate = {2023-07-14},
  isbn = {978-3-95977-226-6}
}

@inproceedings{aminOnlineLearningProfit2015,
  title = {Online {{Learning}} and {{Profit Maximization}} from {{Revealed Preferences}}},
  booktitle = {Proceedings of the {{Twenty-Ninth AAAI Conference}} on {{Artificial Intelligence}}, {{January}} 25-30, 2015, {{Austin}}, {{Texas}}, {{USA}}},
  author = {Amin, Kareem and Cummings, Rachel and Dworkin, Lili and Kearns, Michael J. and Roth, Aaron},
  date = {2015},
  pages = {770--776},
  publisher = {{AAAI Press}},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9984},
  urldate = {2023-07-17}
}

@online{angwinMachineBias2016,
  title = {Machine {{Bias}}},
  author = {Angwin, Julia and Larson, Jeff and Kirchner, Lauren and Mattu, Surya},
  date = {2016-05-23},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  urldate = {2023-08-28},
  abstract = {There’s software used across the country to predict future criminals. And it’s biased against blacks.},
  langid = {english},
  organization = {{ProPublica}}
}

@inproceedings{balcanLearningEconomicParameters2014,
  title = {Learning {{Economic Parameters}} from {{Revealed Preferences}}},
  booktitle = {Web and {{Internet Economics}} - 10th {{International Conference}}, {{WINE}} 2014, {{Beijing}}, {{China}}, {{December}} 14-17, 2014. {{Proceedings}}},
  author = {Balcan, Maria-Florina and Daniely, Amit and Mehta, Ruta and Urner, Ruth and Vazirani, Vijay V.},
  date = {2014},
  volume = {8877},
  pages = {338--353},
  publisher = {{Springer}},
  doi = {10.1007/978-3-319-13129-0_28},
  urldate = {2023-07-17}
}

@inproceedings{barocasHiddenAssumptionsCounterfactual2020,
  title = {The Hidden Assumptions behind Counterfactual Explanations and Principal Reasons},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
  date = {2020-01-27},
  pages = {80--89},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372830},
  urldate = {2023-08-29},
  abstract = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others. These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.},
  isbn = {978-1-4503-6936-7}
}

@inproceedings{bechavodGamingHelpsLearning2021,
  title = {Gaming {{Helps}}! {{Learning}} from {{Strategic Interactions}} in {{Natural Dynamics}}},
  booktitle = {The 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}, {{AISTATS}} 2021, {{April}} 13-15, 2021, {{Virtual Event}}},
  author = {Bechavod, Yahav and Ligett, Katrina and Wu, Zhiwei Steven and Ziani, Juba},
  date = {2021},
  volume = {130},
  pages = {1234--1242},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v130/bechavod21a.html},
  urldate = {2023-07-17}
}

@inproceedings{bechavodInformationDiscrepancyStrategic2022,
  title = {Information {{Discrepancy}} in {{Strategic Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}, {{ICML}} 2022, 17-23 {{July}} 2022, {{Baltimore}}, {{Maryland}}, {{USA}}},
  author = {Bechavod, Yahav and Podimata, Chara and Wu, Zhiwei Steven and Ziani, Juba},
  date = {2022},
  volume = {162},
  pages = {1691--1715},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v162/bechavod22a.html}
}

@inproceedings{beigmanLearningRevealedPreference2006,
  title = {Learning from Revealed Preference},
  booktitle = {Proceedings of the 7th {{ACM}} Conference on {{Electronic}} Commerce},
  author = {Beigman, Eyal and Vohra, Rakesh},
  date = {2006-06-11},
  pages = {36--42},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1134707.1134712},
  urldate = {2023-07-17},
  abstract = {A sequence of prices and demands are rationalizable if there exists a concave, continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price. Afriat [1] presented necessary and sufficient conditions for a finite sequence to be rationalizable. Varian [20] and later Blundell et al. [3, 4] continued this line of work studying nonparametric methods to forecasts demand. Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast. The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts.Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable, but that there exist interesting and potentially useful classes that are learnable from finite samples. We also present a learning algorithm that is an adaptation of a new proof of Afriat's theorem due to Teo and Vohra [17].},
  isbn = {978-1-59593-236-5}
}

@article{canalOneAllSimultaneous2022,
  title = {One for {{All}}: {{Simultaneous Metric}} and {{Preference Learning}} over {{Multiple Users}}},
  shorttitle = {One for {{All}}},
  author = {Canal, Gregory and Mason, Blake and Korlakai Vinayak, Ramya and Nowak, Robert},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {4943--4956},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/1fd4367793bcd3ad38a0b820fcc1b815-Abstract-Conference.html},
  urldate = {2023-07-17},
  langid = {english}
}

@article{caronEfficientBayesianInference2012,
  title = {Efficient {{Bayesian Inference}} for {{Generalized Bradley}}—{{Terry Models}}},
  author = {Caron, François and Doucet, Arnaud},
  date = {2012},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {21},
  number = {1},
  eprint = {23248829},
  eprinttype = {jstor},
  pages = {174--196},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]}},
  issn = {1061-8600},
  url = {https://www.jstor.org/stable/23248829},
  urldate = {2023-09-08},
  abstract = {The Bradley—Terry model is a popular approach to describe probabilities of the possible outcomes when elements of a set are repeatedly compared with one another in pairs. It has found many applications including animal behavior, chess ranking, and multiclass classification. Numerous extensions of the basic model have also been proposed in the literature including models with ties, multiple comparisons, group comparisons, and random graphs. From a computational point of view, Hunter has proposed efficient iterative minorization-maximization (MM) algorithms to perform maximum likelihood estimation for these generalized Bradley—Terry models whereas Bayesian inference is typically performed using Markov chain Monte Carlo algorithms based on tailored Metropolis—Hastings proposals. We show here that these MM algorithms can be reinterpreted as special instances of expectation-maximization algorithms associated with suitable sets of latent variables and propose some original extensions. These latent variables allow us to derive simple Gibbs samplers for Bayesian inference. We demonstrate experimentally the efficiency of these algorithms on a variety of applications.}
}

@article{celarHowPeopleReason2023,
  title = {How People Reason with Counterfactual and Causal Explanations for {{Artificial Intelligence}} Decisions in Familiar and Unfamiliar Domains},
  author = {Celar, Lenart and Byrne, Ruth M. J.},
  date = {2023-03-24},
  journaltitle = {Memory \& Cognition},
  shortjournal = {Mem Cogn},
  issn = {1532-5946},
  doi = {10.3758/s13421-023-01407-5},
  urldate = {2023-08-28},
  abstract = {Few empirical studies have examined how people understand counterfactual explanations for other people’s decisions, for example, “if you had asked for a lower amount, your loan application would have been approved”. Yet many current Artificial Intelligence (AI) decision support systems rely on counterfactual explanations to improve human understanding and trust. We compared counterfactual explanations to causal ones, i.e., “because you asked for a high amount, your loan application was not approved”, for an AI’s decisions in a familiar domain (alcohol and driving) and an unfamiliar one (chemical safety) in four experiments (n\,=\,731). Participants were shown inputs to an AI system, its decisions, and an explanation for each decision; they attempted to predict the AI’s decisions, or to make their own decisions. Participants judged counterfactual explanations more helpful than causal ones, but counterfactuals did not improve the accuracy of their predictions of the AI’s decisions more than causals (Experiment 1). However, counterfactuals improved the accuracy of participants’ own decisions more than causals (Experiment 2). When the AI’s decisions were correct (Experiments 1 and 2), participants considered explanations more helpful and made more accurate judgements in the familiar domain than in the unfamiliar one; but when the AI’s decisions were incorrect, they considered explanations less helpful and made fewer accurate judgements in the familiar domain than the unfamiliar one, whether they predicted the AI’s decisions (Experiment 3a) or made their own decisions (Experiment 3b). The results corroborate the proposal that counterfactuals provide richer information than causals, because their mental representation includes more possibilities.},
  langid = {english}
}

@article{chen2023learning,
  title = {Learning to Incentivize Improvements from Strategic Agents},
  author = {Chen, Yatong and Wang, Jialu and Liu, Yang},
  date = {2023},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=W98AEKQ38Y}
}

@inproceedings{chenLearningStrategyAwareLinear2020,
  title = {Learning {{Strategy-Aware Linear Classifiers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33},
  author = {Chen, Yiling and Liu, Yang and Podimata, Chara},
  date = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/ae87a54e183c075c494c4d397d126a66-Abstract.html}
}

@online{detoniPersonalizedAlgorithmicRecourse2023,
  title = {Personalized {{Algorithmic Recourse}} with {{Preference Elicitation}}},
  author = {De Toni, Giovanni and Viappiani, Paolo and Teso, Stefano and Lepri, Bruno and Passerini, Andrea},
  date = {2023-05-31},
  eprint = {2205.13743},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.13743},
  urldate = {2023-09-06},
  abstract = {Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Reinforcement Learning agent coupled with Monte Carlo Tree Search to quickly identify promising recourse plans. Our empirical evaluation on real-world datasets highlights how PEAR produces high-quality personalized recourse in only a handful of iterations.},
  pubstate = {preprint}
}

@online{dhurandharModelAgnosticContrastive2019b,
  title = {Model {{Agnostic Contrastive Explanations}} for {{Structured Data}}},
  author = {Dhurandhar, Amit and Pedapati, Tejaswini and Balakrishnan, Avinash and Chen, Pin-Yu and Shanmugam, Karthikeyan and Puri, Ruchir},
  date = {2019-05-31},
  eprint = {1906.00117},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1906.00117},
  urldate = {2023-09-08},
  abstract = {Recently, a method [7] was proposed to generate contrastive explanations for differentiable models such as deep neural networks, where one has complete access to the model. In this work, we propose a method, Model Agnostic Contrastive Explanations Method (MACEM), to generate contrastive explanations for \textbackslash emph\{any\} classification model where one is able to \textbackslash emph\{only\} query the class probabilities for a desired input. This allows us to generate contrastive explanations for not only neural networks, but models such as random forests, boosted trees and even arbitrary ensembles that are still amongst the state-of-the-art when learning on structured data [13]. Moreover, to obtain meaningful explanations we propose a principled approach to handle real and categorical features leading to novel formulations for computing pertinent positives and negatives that form the essence of a contrastive explanation. A detailed treatment of the different data types of this nature was not performed in the previous work, which assumed all features to be positive real valued with zero being indicative of the least interesting value. We part with this strong implicit assumption and generalize these methods so as to be applicable across a much wider range of problem settings. We quantitatively and qualitatively validate our approach over 5 public datasets covering diverse domains.},
  pubstate = {preprint}
}

@inproceedings{dongStrategicClassificationRevealed2018,
  title = {Strategic {{Classification}} from {{Revealed Preferences}}},
  booktitle = {Proceedings of the 2018 {{ACM Conference}} on {{Economics}} and {{Computation}}},
  author = {Dong, Jinshuo and Roth, Aaron and Schutzman, Zachary and Waggoner, Bo and Wu, Zhiwei Steven},
  date = {2018},
  pages = {55--70},
  location = {{Ithaca, NY, USA}},
  doi = {10.1145/3219166.3219193}
}

@article{eberhardtInterventionsCausalInference2007,
  title = {Interventions and {{Causal Inference}}},
  author = {Eberhardt, Frederick and Scheines, Richard},
  date = {2007},
  journaltitle = {Philosophy of Science},
  volume = {74},
  number = {5},
  eprint = {10.1086/525638},
  eprinttype = {jstor},
  pages = {981--995},
  publisher = {{[The University of Chicago Press, Philosophy of Science Association]}},
  issn = {0031-8248},
  doi = {10.1086/525638},
  urldate = {2023-08-18},
  abstract = {The literature on causal discovery has focused on interventions that involve randomly assigning values to a single variable. But such a randomized intervention is not the only possibility, nor is it always optimal. In some cases it is impossible or it would be unethical to perform such an intervention. We provide an account of ‘hard’ and ‘soft’ interventions and discuss what they can contribute to causal discovery. We also describe how the choice of the optimal intervention(s) depends heavily on the particular experimental setup and the assumptions that can be made.}
}

@inproceedings{eilatStrategicClassificationGraph2023,
  title = {Strategic {{Classification}} with {{Graph Neural Networks}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}, {{ICLR}} 2023, {{Kigali}}, {{Rwanda}}, {{May}} 1-5, 2023},
  author = {Eilat, Itay and Finkelshtein, Ben and Baskin, Chaim and Rosenfeld, Nir},
  date = {2023},
  url = {https://openreview.net/pdf?id=TuHkVOjSAR}
}

@inproceedings{ghalmeStrategicClassificationDark2021,
  title = {Strategic {{Classification}} in the {{Dark}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2021, 18-24 {{July}} 2021, {{Virtual Event}}},
  author = {Ghalme, Ganesh and Nair, Vineet and Eilat, Itay and Talgam-Cohen, Inbal and Rosenfeld, Nir},
  date = {2021},
  volume = {139},
  pages = {3672--3681},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v139/ghalme21a.html}
}

@article{groteEthicsAlgorithmicDecisionmaking2020,
  title = {On the Ethics of Algorithmic Decision-Making in Healthcare},
  author = {Grote, Thomas and Berens, Philipp},
  date = {2020-03-01},
  journaltitle = {Journal of Medical Ethics},
  volume = {46},
  number = {3},
  eprint = {31748206},
  eprinttype = {pmid},
  pages = {205--211},
  publisher = {{Institute of Medical Ethics}},
  issn = {0306-6800, 1473-4257},
  doi = {10.1136/medethics-2019-105586},
  urldate = {2023-08-28},
  abstract = {In recent years, a plethora of high-profile scientific publications has been reporting about machine learning algorithms outperforming clinicians in medical diagnosis or treatment recommendations. This has spiked interest in deploying relevant algorithms with the aim of enhancing decision-making in healthcare. In this paper, we argue that instead of straightforwardly enhancing the decision-making capabilities of clinicians and healthcare institutions, deploying machines learning algorithms entails trade-offs at the epistemic and the normative level. Whereas involving machine learning might improve the accuracy of medical diagnosis, it comes at the expense of opacity when trying to assess the reliability of given diagnosis. Drawing on literature in social epistemology and moral responsibility, we argue that the uncertainty in question potentially undermines the epistemic authority of clinicians. Furthermore, we elucidate potential pitfalls of involving machine learning in healthcare with respect to paternalism, moral responsibility and fairness. At last, we discuss how the deployment of machine learning algorithms might shift the evidentiary norms of medical diagnosis. In this regard, we hope to lay the grounds for further ethical reflection of the opportunities and pitfalls of machine learning for enhancing decision-making in healthcare.},
  langid = {english}
}

@inproceedings{hardtStrategicClassification2016,
  title = {Strategic {{Classification}}},
  booktitle = {Proceedings of the 2016 {{ACM Conference}} on {{Innovations}} in {{Theoretical Computer Science}}},
  author = {Hardt, Moritz and Megiddo, Nimrod and Papadimitriou, Christos and Wootters, Mary},
  date = {2016},
  pages = {111--122},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2840728.2840730},
  venue = {Cambridge, Massachusetts, USA}
}

@inproceedings{harrisStrategicInstrumentalVariable2022,
  title = {Strategic {{Instrumental Variable Regression}}: {{Recovering Causal Relationships From Strategic Responses}}},
  shorttitle = {Strategic {{Instrumental Variable Regression}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}, {{ICML}} 2022, 17-23 {{July}} 2022, {{Baltimore}}, {{Maryland}}, {{USA}}},
  author = {Harris, Keegan and Ngo, Dung Daniel T. and Stapleton, Logan and Heidari, Hoda and Wu, Steven},
  date = {2022},
  volume = {162},
  pages = {8502--8522},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v162/harris22a.html},
  urldate = {2023-07-17}
}

@online{horowitzCausalStrategicClassification2023,
  title = {Causal {{Strategic Classification}}: {{A Tale}} of {{Two Shifts}}},
  shorttitle = {Causal {{Strategic Classification}}},
  author = {Horowitz, Guy and Rosenfeld, Nir},
  date = {2023-06-09},
  eprint = {2302.06280},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.06280},
  urldate = {2023-07-17},
  abstract = {When users can benefit from certain predictive outcomes, they may be prone to act to achieve those outcome, e.g., by strategically modifying their features. The goal in strategic classification is therefore to train predictive models that are robust to such behavior. However, the conventional framework assumes that changing features does not change actual outcomes, which depicts users as "gaming" the system. Here we remove this assumption, and study learning in a causal strategic setting where true outcomes do change. Focusing on accuracy as our primary objective, we show how strategic behavior and causal effects underlie two complementing forms of distribution shift. We characterize these shifts, and propose a learning algorithm that balances between these two forces and over time, and permits end-to-end training. Experiments on synthetic and semi-synthetic data demonstrate the utility of our approach.},
  pubstate = {preprint}
}

@article{imbensPotentialOutcomeDirected2020,
  title = {Potential {{Outcome}} and {{Directed Acyclic Graph Approaches}} to {{Causality}}: {{Relevance}} for {{Empirical Practice}} in {{Economics}}},
  shorttitle = {Potential {{Outcome}} and {{Directed Acyclic Graph Approaches}} to {{Causality}}},
  author = {Imbens, Guido W.},
  date = {2020-12},
  journaltitle = {Journal of Economic Literature},
  volume = {58},
  number = {4},
  pages = {1129--1179},
  issn = {0022-0515},
  doi = {10.1257/jel.20191597},
  urldate = {2023-09-07},
  abstract = {(Pearl and Mackenzie 2018). I also discuss the potential outcome framework developed by Rubin and coauthors (e.g., Rubin 2006), building on work by Neyman (1990 [1923]). I then discuss the relative merits of these approaches for empirical work in economics, focusing on the questions each framework answers well, and why much of the the work in economics is closer in spirit to the potential outcome perspective.},
  langid = {english}
}

@inproceedings{jagadeesanAlternativeMicrofoundationsStrategic2021,
  title = {Alternative {{Microfoundations}} for {{Strategic Classification}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2021, 18-24 {{July}} 2021, {{Virtual Event}}},
  author = {Jagadeesan, Meena and Mendler-Dünner, Celestine and Hardt, Moritz},
  date = {2021},
  volume = {139},
  pages = {4687--4697},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v139/jagadeesan21a.html}
}

@inproceedings{karimiAlgorithmicRecourseCounterfactual2021,
  title = {Algorithmic {{Recourse}}: {{From Counterfactual Explanations}} to {{Interventions}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Karimi, Amir-Hossein and Schölkopf, Bernhard and Valera, Isabel},
  date = {2021},
  pages = {353--362},
  location = {{New York, NY, USA}},
  doi = {10.1145/3442188.3445899},
  venue = {Virtual Event, Canada}
}

@inproceedings{karimiAlgorithmicRecourseImperfect2020,
  title = {Algorithmic Recourse under Imperfect Causal Knowledge: A Probabilistic Approach},
  shorttitle = {Algorithmic Recourse under Imperfect Causal Knowledge},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Karimi, Amir-Hossein and family=Kügelgen, given=Julius, prefix=von, useprefix=true and Schölkopf, Bernhard and Valera, Isabel},
  date = {2020},
  volume = {33},
  pages = {265--277},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/02a3c7fb3f489288ae6942498498db20-Abstract.html},
  urldate = {2023-08-30},
  abstract = {Recent work has discussed the limitations of counterfactual explanations to recommend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we first show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (e.g., only the causal graph). The first captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines.}
}

@article{karimiSurveyAlgorithmicRecourse2022,
  title = {A {{Survey}} of {{Algorithmic Recourse}}: {{Contrastive Explanations}} and {{Consequential Recommendations}}},
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Schölkopf, Bernhard and Valera, Isabel},
  date = {2022},
  journaltitle = {ACM Comput. Surv.},
  volume = {55},
  number = {5},
  location = {{New York, NY, USA}},
  doi = {10.1145/3527848}
}

@online{kramerProblemsAIVideo2022,
  title = {The Problems with {{AI}} Video Interviews},
  author = {Kramer, Anna},
  date = {2022-05-27},
  url = {https://www.protocol.com/workplace/automated-video-interviews-hirevue-modernhire},
  urldate = {2023-08-29},
  abstract = {The automated video interview has become increasingly popular. There are nearly endless options with similar services — among them HireVue, Modern Hire, Spark Hire, myInterview, Humanly.io, Willo and Curious Thing. Entry-level college graduates in banking, tech and even consulting almost always get funneled through these systems.},
  langid = {english},
  organization = {{Protocol}}
}

@article{vonkugelgenFairnessCausalAlgorithmic2022,
  title = {On the {{Fairness}} of {{Causal Algorithmic Recourse}}},
  author = {family=Kügelgen, given=Julius, prefix=von, useprefix=true and Karimi, Amir-Hossein and Bhatt, Umang and Valera, Isabel and Weller, Adrian and Schölkopf, Bernhard},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {9},
  pages = {9584--9594},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i9.21192},
  urldate = {2023-09-08},
  abstract = {Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which—unlike prior work on equalising the average group-wise distance from the decision boundary—explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier.},
  issue = {9},
  langid = {english}
}

@inproceedings{levanonGeneralizedStrategicClassification2022,
  title = {Generalized {{Strategic Classification}} and the {{Case}} of {{Aligned Incentives}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}, {{ICML}} 2022, 17-23 {{July}} 2022, {{Baltimore}}, {{Maryland}}, {{USA}}},
  author = {Levanon, Sagi and Rosenfeld, Nir},
  date = {2022},
  volume = {162},
  pages = {12593--12618},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v162/levanon22a.html}
}

@inproceedings{levanonStrategicClassificationMade2021,
  title = {Strategic {{Classification Made Practical}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2021, 18-24 {{July}} 2021, {{Virtual Event}}},
  author = {Levanon, Sagi and Rosenfeld, Nir},
  date = {2021},
  volume = {139},
  pages = {6243--6253},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v139/levanon21a.html}
}

@inproceedings{mendler-dunnerAnticipatingPerformativityPredicting2022,
  title = {Anticipating {{Performativity}} by {{Predicting}} from {{Predictions}}},
  booktitle = {{{NeurIPS}}},
  author = {Mendler-Dünner, Celestine and Ding, Frances and Wang, Yixin},
  date = {2022},
  url = {http://papers.nips.cc/paper\_files/paper/2022/hash/ca09b375e8e2b2c789698c079a9fc51c-Abstract-Conference.html},
  urldate = {2023-07-17}
}

@inproceedings{millerStrategicClassificationCausal2020,
  title = {Strategic {{Classification}} Is {{Causal Modeling}} in {{Disguise}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2020, 13-18 {{July}} 2020, {{Virtual Event}}},
  author = {Miller, John and Milli, Smitha and Hardt, Moritz},
  date = {2020},
  volume = {119},
  pages = {6917--6926},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v119/miller20b.html},
  urldate = {2023-07-17}
}

@inproceedings{nairStrategicRepresentation2022,
  title = {Strategic {{Representation}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}, {{ICML}} 2022, 17-23 {{July}} 2022, {{Baltimore}}, {{Maryland}}, {{USA}}},
  author = {Nair, Vineet and Ghalme, Ganesh and Talgam-Cohen, Inbal and Rosenfeld, Nir},
  date = {2022},
  volume = {162},
  pages = {16331--16352},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v162/nair22a.html}
}

@article{odonoghueOperatorSplittingHomogeneous2021,
  title = {Operator {{Splitting}} for a {{Homogeneous Embedding}} of the {{Linear Complementarity Problem}}},
  author = {O'Donoghue, Brendan},
  date = {2021},
  journaltitle = {SIAM J. Optim.},
  volume = {31},
  number = {3},
  pages = {1999--2023},
  doi = {10.1137/20M1366307}
}

@online{odwyerAreYouCreditworthy2018,
  title = {Are {{You Creditworthy}}? {{The Algorithm Will Decide}}.},
  shorttitle = {Are {{You Creditworthy}}?},
  author = {O’Dwyer, Rachel},
  date = {2018-05-07T12:12:56+00:00},
  url = {https://undark.org/2018/05/07/algorithmic-credit-scoring-machine-learning/},
  urldate = {2023-08-28},
  abstract = {Whether we ought to have faith in algorithmic credit scoring is hard to answer, given the impenetrability of machine learning.},
  langid = {american},
  organization = {{Undark Magazine}}
}

@book{pearl2016causal,
  title = {Causal {{Inference}} in {{Statistics}}: {{A Primer}}},
  author = {Pearl, J. and Glymour, M. and Jewell, N.P.},
  date = {2016},
  publisher = {{Wiley}},
  isbn = {978-1-119-18684-7},
  lccn = {2015037219}
}

@inproceedings{prilloSoftSortContinuousRelaxation2020,
  title = {{{SoftSort}}: {{A Continuous Relaxation}} for the Argsort {{Operator}}},
  shorttitle = {{{SoftSort}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2020, 13-18 {{July}} 2020, {{Virtual Event}}},
  author = {Prillo, Sebastian and Eisenschlos, Julian Martin},
  date = {2020},
  volume = {119},
  pages = {7793--7802},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v119/prillo20a.html},
  urldate = {2023-08-20}
}

@article{ramakrishnanSynthesizingActionSequences2020,
  title = {Synthesizing {{Action Sequences}} for {{Modifying Model Decisions}}},
  author = {Ramakrishnan, Goutham and Lee, Yun Chan and Albarghouthi, Aws},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {5462--5469},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.5996},
  urldate = {2023-08-29},
  abstract = {When a model makes a consequential decision, e.g., denying someone a loan, it needs to additionally generate actionable, realistic feedback on what the person can do to favorably change the decision. We cast this problem through the lens of program synthesis, in which our goal is to synthesize an optimal (realistically cheapest or simplest) sequence of actions that if a person executes successfully can change their classification. We present a novel and general approach that combines search-based program synthesis and test-time adversarial attacks to construct action sequences over a domain-specific set of actions. We demonstrate the effectiveness of our approach on a number of deep neural networks.},
  issue = {04},
  langid = {english}
}

@inproceedings{rawalIndividualizedRecourseInterpretable2020,
  title = {Beyond {{Individualized Recourse}}: {{Interpretable}} and {{Interactive Summaries}} of {{Actionable Recourses}}},
  shorttitle = {Beyond {{Individualized Recourse}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rawal, Kaivalya and Lakkaraju, Himabindu},
  date = {2020},
  volume = {33},
  pages = {12187--12198},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/8ee7730e97c67473a424ccfeff49ab20-Abstract.html},
  urldate = {2023-09-08},
  abstract = {As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyze and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population.  We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population. More specifically, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well defined subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination.}
}

@inproceedings{rothWatchLearnOptimizing2016,
  title = {Watch and Learn: Optimizing from Revealed Preferences Feedback},
  shorttitle = {Watch and Learn},
  booktitle = {Proceedings of the 48th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}, {{STOC}} 2016, {{Cambridge}}, {{MA}}, {{USA}}, {{June}} 18-21, 2016},
  author = {Roth, Aaron and Ullman, Jonathan R. and Wu, Zhiwei Steven},
  date = {2016},
  pages = {949--962},
  publisher = {{ACM}},
  doi = {10.1145/2897518.2897579}
}

@article{samuelsonConsumptionTheoryTerms1948,
  title = {Consumption {{Theory}} in {{Terms}} of {{Revealed Preference}}},
  author = {Samuelson, Paul},
  date = {1948},
  journaltitle = {Economica},
  volume = {15},
  number = {60},
  eprint = {2549561},
  eprinttype = {jstor},
  pages = {243--253},
  publisher = {{[London School of Economics, Wiley, London School of Economics and Political Science, Suntory and Toyota International Centres for Economics and Related Disciplines]}},
  issn = {0013-0427},
  doi = {10.2307/2549561},
  urldate = {2023-08-23}
}

@article{samuelsonNotePureTheory1938,
  title = {A {{Note}} on the {{Pure Theory}} of {{Consumer}}'s {{Behaviour}}},
  author = {Samuelson, Paul},
  date = {1938},
  journaltitle = {Economica},
  volume = {5},
  number = {17},
  eprint = {2548836},
  eprinttype = {jstor},
  pages = {61--71},
  publisher = {{[London School of Economics, Wiley, London School of Economics and Political Science, Suntory and Toyota International Centres for Economics and Related Disciplines]}},
  issn = {0013-0427},
  doi = {10.2307/2548836},
  urldate = {2023-08-23}
}

@inproceedings{shavitCausalStrategicLinear2020,
  title = {Causal {{Strategic Linear Regression}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2020, 13-18 {{July}} 2020, {{Virtual Event}}},
  author = {Shavit, Yonadav and Edelman, Benjamin L. and Axelrod, Brian},
  date = {2020},
  volume = {119},
  pages = {8676--8686},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v119/shavit20a.html},
  urldate = {2023-07-17}
}

@inproceedings{ustunActionableRecourseLinear2019,
  title = {Actionable {{Recourse}} in {{Linear Classification}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
  date = {2019-01-29},
  pages = {10--19},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287566},
  urldate = {2023-08-19},
  abstract = {Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.},
  isbn = {978-1-4503-6125-5}
}

@inproceedings{vanlooverenInterpretableCounterfactualExplanations2021,
  title = {Interpretable {{Counterfactual Explanations Guided}} by {{Prototypes}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author = {Van Looveren, Arnaud and Klaise, Janis},
  date = {2021},
  pages = {650--665},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-86520-7_40},
  abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the search for counterfactual instances and result in more interpretable explanations. We quantitatively evaluate interpretability of the generated counterfactuals to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). Additionally, we propose a principled approach to handle categorical variables and illustrate our method on the Adult (Census) dataset. Our method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for black box models.},
  isbn = {978-3-030-86520-7},
  langid = {english}
}

@inproceedings{venkatasubramanianPhilosophicalBasisAlgorithmic2020,
  title = {The Philosophical Basis of Algorithmic Recourse},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Venkatasubramanian, Suresh and Alfano, Mark},
  date = {2020-01-27},
  pages = {284--293},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372876},
  urldate = {2023-08-29},
  abstract = {Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse. We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.},
  isbn = {978-1-4503-6936-7}
}

@book{voigtEUGeneralData2017,
  title = {The {{EU General Data Protection Regulation}} ({{GDPR}}): {{A Practical Guide}}},
  author = {Voigt, Paul and family=Bussche, given=Axel, prefix=von dem, useprefix=false},
  date = {2017},
  edition = {1},
  publisher = {{Springer Publishing Company, Incorporated}},
  doi = {10.1007/978-3-319-57959-7}
}

@inproceedings{zadimoghaddamEfficientlyLearningRevealed2012,
  title = {Efficiently {{Learning}} from {{Revealed Preference}}},
  booktitle = {Internet and {{Network Economics}} - 8th {{International Workshop}}, {{WINE}} 2012, {{Liverpool}}, {{UK}}, {{December}} 10-12, 2012. {{Proceedings}}},
  author = {Zadimoghaddam, Morteza and Roth, Aaron},
  date = {2012},
  volume = {7695},
  pages = {114--127},
  publisher = {{Springer}},
  doi = {10.1007/978-3-642-35311-6_9},
  urldate = {2023-07-17}
}

@inproceedings{zrnicWhoLeadsWho2021,
  title = {Who {{Leads}} and {{Who Follows}} in {{Strategic Classification}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 34},
  author = {Zrnic, Tijana and Mazumdar, Eric and Sastry, S. Shankar and Jordan, Michael I.},
  date = {2021},
  pages = {15257--15269},
  url = {https://proceedings.neurips.cc/paper/2021/hash/812214fb8e7066bfa6e32c626c2c688b-Abstract.html}
}
